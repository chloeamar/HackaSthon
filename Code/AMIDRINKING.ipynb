{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Student.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook for the Hackathon"
      ],
      "metadata": {
        "id": "Zkm5ombQnjv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you will find all the provided code for the Hackathon. Do not forget to check the github page https://github.com/danielAmar02/HackaSthon for data and information. \n",
        "\n",
        "Enjoy and ask many questions, we are here to help !!"
      ],
      "metadata": {
        "id": "bxylRSQQnjs-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xY4RylP5nWsN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, BatchNormalization\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from keras.regularizers import l2\n",
        "from time import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip D.zip "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd55z3Y7nrG8",
        "outputId": "0adee1a2-88c3-4c7a-8fd3-7c66f45b84f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  D.zip\n",
            "   creating: Drink/\n",
            "  inflating: Drink/LOG_000.CSV       \n",
            "  inflating: Drink/LOG_001.CSV       \n",
            "  inflating: Drink/LOG_002.CSV       \n",
            "  inflating: Drink/LOG_003.CSV       \n",
            "  inflating: Drink/LOG_004.CSV       \n",
            "  inflating: Drink/LOG_005.CSV       \n",
            "  inflating: Drink/LOG_006.CSV       \n",
            "  inflating: Drink/LOG_007.CSV       \n",
            "  inflating: Drink/LOG_008.CSV       \n",
            "  inflating: Drink/LOG_009.CSV       \n",
            "  inflating: Drink/LOG_010.CSV       \n",
            "  inflating: Drink/LOG_011.CSV       \n",
            "  inflating: Drink/LOG_012.CSV       \n",
            "  inflating: Drink/LOG_013.CSV       \n",
            "  inflating: Drink/LOG_014.CSV       \n",
            "  inflating: Drink/LOG_015.CSV       \n",
            "  inflating: Drink/LOG_016.CSV       \n",
            "  inflating: Drink/LOG_017.CSV       \n",
            "  inflating: Drink/LOG_018.CSV       \n",
            "  inflating: Drink/LOG_019.CSV       \n",
            "  inflating: Drink/LOG_020.CSV       \n",
            "  inflating: Drink/LOG_021.CSV       \n",
            "  inflating: Drink/LOG_022.CSV       \n",
            "  inflating: Drink/LOG_023.CSV       \n",
            "  inflating: Drink/LOG_024.CSV       \n",
            "  inflating: Drink/LOG_025.CSV       \n",
            "  inflating: Drink/LOG_026.CSV       \n",
            "  inflating: Drink/LOG_027.CSV       \n",
            "  inflating: Drink/LOG_028.CSV       \n",
            "  inflating: Drink/LOG_029.CSV       \n",
            "  inflating: Drink/LOG_030.CSV       \n",
            "  inflating: Drink/LOG_031.CSV       \n",
            "  inflating: Drink/LOG_032.CSV       \n",
            "  inflating: Drink/LOG_033.CSV       \n",
            "  inflating: Drink/LOG_034.CSV       \n",
            "  inflating: Drink/LOG_035.CSV       \n",
            "  inflating: Drink/LOG_036.CSV       \n",
            "  inflating: Drink/LOG_037.CSV       \n",
            "  inflating: Drink/LOG_038.CSV       \n",
            "  inflating: Drink/LOG_039.CSV       \n",
            "  inflating: Drink/LOG_040.CSV       \n",
            "  inflating: Drink/LOG_041.CSV       \n",
            "  inflating: Drink/LOG_042.CSV       \n",
            "  inflating: Drink/LOG_043.CSV       \n",
            "  inflating: Drink/LOG_044.CSV       \n",
            "  inflating: Drink/LOG_045.CSV       \n",
            "  inflating: Drink/LOG_046.CSV       \n",
            "  inflating: Drink/LOG_047.CSV       \n",
            "  inflating: Drink/LOG_048.CSV       \n",
            "  inflating: Drink/LOG_049.CSV       \n",
            "  inflating: Drink/LOG_050.CSV       \n",
            "  inflating: Drink/LOG_051.CSV       \n",
            "  inflating: Drink/LOG_052.CSV       \n",
            "  inflating: Drink/LOG_053.CSV       \n",
            "  inflating: Drink/LOG_054.CSV       \n",
            "  inflating: Drink/LOG_055.CSV       \n",
            "  inflating: Drink/LOG_056.CSV       \n",
            "  inflating: Drink/LOG_057.CSV       \n",
            "  inflating: Drink/LOG_058.CSV       \n",
            "  inflating: Drink/LOG_059.CSV       \n",
            "  inflating: Drink/LOG_060.CSV       \n",
            "  inflating: Drink/LOG_061.CSV       \n",
            "  inflating: Drink/LOG_062.CSV       \n",
            "  inflating: Drink/LOG_063.CSV       \n",
            "  inflating: Drink/LOG_064.CSV       \n",
            "  inflating: Drink/LOG_065.CSV       \n",
            "  inflating: Drink/LOG_066.CSV       \n",
            "   creating: normal/\n",
            "  inflating: normal/LOG_000.CSV      \n",
            "  inflating: normal/LOG_001.CSV      \n",
            "  inflating: normal/LOG_002.CSV      \n",
            "  inflating: normal/LOG_003.CSV      \n",
            "  inflating: normal/LOG_004.CSV      \n",
            "  inflating: normal/LOG_005.CSV      \n",
            "  inflating: normal/LOG_006.CSV      \n",
            "  inflating: normal/LOG_007.CSV      \n",
            "  inflating: normal/LOG_008.CSV      \n",
            "  inflating: normal/LOG_009.CSV      \n",
            "  inflating: normal/LOG_010.CSV      \n",
            "  inflating: normal/LOG_011.CSV      \n",
            "  inflating: normal/LOG_012.CSV      \n",
            "  inflating: normal/LOG_013.CSV      \n",
            "  inflating: normal/LOG_014.CSV      \n",
            "  inflating: normal/LOG_015.CSV      \n",
            "  inflating: normal/LOG_016.CSV      \n",
            "  inflating: normal/LOG_017.CSV      \n",
            "  inflating: normal/LOG_018.CSV      \n",
            "  inflating: normal/LOG_019.CSV      \n",
            "  inflating: normal/LOG_020.CSV      \n",
            "  inflating: normal/LOG_021.CSV      \n",
            "  inflating: normal/LOG_022.CSV      \n",
            "  inflating: normal/LOG_023.CSV      \n",
            "  inflating: normal/LOG_024.CSV      \n",
            "  inflating: normal/LOG_025.CSV      \n",
            "  inflating: normal/LOG_026.CSV      \n",
            "  inflating: normal/LOG_027.CSV      \n",
            "  inflating: normal/LOG_028.CSV      \n",
            "  inflating: normal/LOG_029.CSV      \n",
            "  inflating: normal/LOG_030.CSV      \n",
            "  inflating: normal/LOG_031.CSV      \n",
            "  inflating: normal/LOG_032.CSV      \n",
            "  inflating: normal/LOG_033.CSV      \n",
            "  inflating: normal/LOG_034.CSV      \n",
            "  inflating: normal/LOG_035.CSV      \n",
            "  inflating: normal/LOG_036.CSV      \n",
            "  inflating: normal/LOG_037.CSV      \n",
            "  inflating: normal/LOG_038.CSV      \n",
            "  inflating: normal/LOG_039.CSV      \n",
            "  inflating: normal/LOG_040.CSV      \n",
            "  inflating: normal/LOG_041.CSV      \n",
            "  inflating: normal/LOG_042.CSV      \n",
            "  inflating: normal/LOG_043.CSV      \n",
            "  inflating: normal/LOG_044.CSV      \n",
            "  inflating: normal/LOG_045.CSV      \n",
            "  inflating: normal/LOG_046.CSV      \n",
            "  inflating: normal/LOG_047.CSV      \n",
            "  inflating: normal/LOG_048.CSV      \n",
            "  inflating: normal/LOG_049.CSV      \n",
            "  inflating: normal/LOG_050.CSV      \n",
            "  inflating: normal/LOG_051.CSV      \n",
            "  inflating: normal/LOG_052.CSV      \n",
            "  inflating: normal/LOG_053.CSV      \n",
            "  inflating: normal/LOG_054.CSV      \n",
            "  inflating: normal/LOG_055.CSV      \n",
            "  inflating: normal/LOG_056.CSV      \n",
            "  inflating: normal/LOG_057.CSV      \n",
            "  inflating: normal/LOG_058.CSV      \n",
            "  inflating: normal/LOG_059.CSV      \n",
            "  inflating: normal/LOG_060.CSV      \n",
            "  inflating: normal/LOG_061.CSV      \n",
            "  inflating: normal/LOG_062.CSV      \n",
            "  inflating: normal/LOG_063.CSV      \n",
            "  inflating: normal/LOG_064.CSV      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drink_file_name=[]\n",
        "normal_file_name=[]\n",
        "directory_drink='/content/Drink'\n",
        "for filename in os.listdir(directory_drink):\n",
        "  f=os.path.join(directory_drink,filename)\n",
        "  drink_file_name+=[f]\n",
        "\n",
        "directory_normal='/content/normal'\n",
        "for filename in os.listdir(directory_normal):\n",
        "  f=os.path.join(directory_normal,filename)\n",
        "  normal_file_name+=[f]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L3S_kVuDnzNH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with Drink file\n",
        "\n",
        "# How to get back the data\n",
        "\n",
        "for i in range(len(drink_file_name)):\n",
        "  filename=drink_file_name[i]\n",
        "  x=[]\n",
        "  y=[]\n",
        "  z=[]\n",
        "  df=pd.read_csv(filename)\n",
        "  for sample in range(len(df.values)):\n",
        "    new_sample=[(lambda x : float(x))(x) for x in df.values[sample][0].split(' ')]\n",
        "    for k in range(0,len(new_sample),3):\n",
        "      x+=[new_sample[k]]\n",
        "      y+=[new_sample[k+1]]\n",
        "      z+=[new_sample[k+2]]"
      ],
      "metadata": {
        "id": "V0f4WlrN0CTp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Plot in 3D some of the samples you generated**"
      ],
      "metadata": {
        "id": "TOIL93UTzT5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot "
      ],
      "metadata": {
        "id": "fbS3Leufn47b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Is the signal noisy ? Why ? Try to make it much more readable.**"
      ],
      "metadata": {
        "id": "IUMv435X0a1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##def smoothing()\n",
        "#Try to smooth x,y,z"
      ],
      "metadata": {
        "id": "xzDRi316n-OH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a Train and a Test datasets**\n",
        "\n",
        "We recommend you to create first a dataset with all the samples. \n",
        "It should be a numpy array of size (Batch= number of samples, sequences, 3)"
      ],
      "metadata": {
        "id": "ynKegYFn0m12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train,y_train,X_test,y_test="
      ],
      "metadata": {
        "id": "JlEf5zQk0khv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For simplicity, normalize your data. Be careful, you should calculte mean and std only on the train dataset.**\n",
        "\n"
      ],
      "metadata": {
        "id": "D-jy4GIw1LWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mean=\n",
        "#std="
      ],
      "metadata": {
        "id": "YTqKBGvo1nP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **According to you, what will be the main difficulties regarding the classification we want to make. Bonus: Provide 2 ways to improve already classification**"
      ],
      "metadata": {
        "id": "6XkIMJWK-Kkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use stastical or Machine learning approaches to classify the data**\n"
      ],
      "metadata": {
        "id": "8R2yH-tl1qP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can try random forest"
      ],
      "metadata": {
        "id": "G5MPTeg_4NiL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comment the results**\n",
        "\n"
      ],
      "metadata": {
        "id": "B_iY--tf6aUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now use Deep Learning and a very simple LSTM model. For more information, https://en.wikipedia.org/wiki/Long_short-term_memory\n"
      ],
      "metadata": {
        "id": "50A9UhIY4N5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make it run** "
      ],
      "metadata": {
        "id": "AUh5j9lk4ZPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length=np.shape(X_train)[1]\n",
        "\n",
        "inputs=keras.Input(shape=(sequence_length,3))\n",
        "x = layers.LSTM(32, recurrent_dropout=0.25)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs= keras.layers.Dense(2, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "7d09jpDi4J7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "1_-N3SC_4rcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        "    \n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=16,\n",
        "    epochs=10,\n",
        "    validation_split=0.2,\n",
        ")\n"
      ],
      "metadata": {
        "id": "GW5ALkEl1vaG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We made a 55%-60\\% prediction which barely outperformed the random threshold of 50\\% (check it). Try to improve it as much as you can. If you don't have time, try to suggest ideas.**\n",
        "\n"
      ],
      "metadata": {
        "id": "gPxXJQZO2651"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A93kSWDd5Jtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eDtF3Ezd5Jp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jYzDu1ch5JmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have finished this, go on https://github.com/danielAmar02/HackaSthon/ for the qualitative questions."
      ],
      "metadata": {
        "id": "Ze5Tkhzq5KLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Rrr5lvRD5b4f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}